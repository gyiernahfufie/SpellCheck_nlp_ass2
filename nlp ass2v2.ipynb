{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a2e4e8-5d78-4964-9576-7c682bdc1bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only run when added new files into corpus \n",
    "import glob\n",
    "\n",
    "read_files = glob.glob(\"*.txt\")\n",
    "\n",
    "with open(\"merged_corpus.txt\", \"wb\") as outfile:\n",
    "    for f in read_files:\n",
    "        with open(f, \"rb\") as infile:\n",
    "            outfile.write(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "361f3b05-be8d-4a17-b227-4116e8a457a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\User\\\\Downloads\\\\NLP ASS 2\\\\NLP_Spellchecker_App-master (1)'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "57a1f32e-b360-4502-8cad-6c2d2ac3952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c3451dfc-5389-442d-8052-a52ea7fa646b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\User\\\\Downloads\\\\NLP ASS 2\\\\NLP_Spellchecker_App-master (1)\\\\corpus\"\n",
    "dirs = os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e05fa553-e39b-44e8-aff1-b9e56629b9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string \n",
    "import matplotlib\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f6cc3e72-010b-47c2-87a8-bae946cd4e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"merged_corpus.txt\", \"r\", encoding = 'ISO-8859-1') as f: \n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "503d39a0-35ba-48a7-a15a-7f4e078131b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#514-8 = proj gutenberg \n",
    "#medical vocab = https://github.com/socd06/medical-nlp\n",
    "#medical articles = https://www.kaggle.com/trikialaaa/2k-clean-medical-articles-medicalnewstoday/tasks\n",
    "#merged corpus = 514-8 + medical articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6b2150de-ff88-44c3-bae9-24910fa84231",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess file    \n",
    "data = re.sub(r'[^A-Za-z\\.\\?!\\']+', ' ', data) #remove special character\n",
    "data = re.sub(r'\\b(?:[A-Z]{3,}\\b|(?:[A-Za-z]\\.){2,})\\s*', ' ',data)# removes 3 or more capital letters e.g. XYZ or X.Y.Z. \n",
    "sentences = re.split(r'[\\.\\?!]+[ \\n]+', data) #split data into sentences\n",
    "sentences = [s.strip() for s in sentences] #Remove leading & trailing spaces\n",
    "sentences = [s for s in sentences if len(s) > 0] #Remove whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7f8daaaf-e8a0-4a2e-91df-b4c2c20a4f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences=[]\n",
    "# this is the new tokenisation for sentences. This will keep all words including \"shouldn't\". Removes numbers and non words like e.g. and etc. \n",
    "for sentence in sentences: \n",
    "        non_words = ['etc'] \n",
    "        # Convert to lowercase letters\n",
    "        sentence = sentence.lower() \n",
    "        \n",
    "        # Convert into a list of words\n",
    "        tokenized = re.findall(\"[\\w']+\", sentence) #tokenises words. keeps numbers keeps shouldn't but has e and g in list from e.g. \n",
    "        \n",
    "        # Remove random letters leftover from tokenising. \n",
    "        one_word = list(string.ascii_lowercase[0:26])\n",
    "        one_word.remove('a')\n",
    "        one_word.remove('i')\n",
    "        \n",
    "        for i in list(tokenized):\n",
    "            if i in one_word:\n",
    "                tokenized.remove(i)\n",
    "                \n",
    "        # remove non_words i.e. 'etc'\n",
    "        for word in list(tokenized): \n",
    "                if word in non_words: \n",
    "                    tokenized.remove(word) \n",
    "        # append the list of tokenized_sentences to the list of lists\n",
    "        tokenized_sentences.append(tokenized) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "995b9e07-26fc-4bdf-b0d7-6c36c6fed866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(tokenized_sentences):\n",
    "    \"\"\"\n",
    "    Count the number of each individual word in every sentences\n",
    "    \"\"\"\n",
    "    word_counts = {}\n",
    "    for sentence in tokenized_sentences: # complete this line\n",
    "        word_counts\n",
    "        # Go through each token in the sentence\n",
    "        for token in sentence: # complete this line\n",
    "\n",
    "            # If the token is not in the dictionary yet, set the count to 1\n",
    "            if not token in word_counts: # complete this line\n",
    "                word_counts[token] = 1\n",
    "            \n",
    "            # If the token is already in the dictionary, increment the count by 1\n",
    "            else:\n",
    "                word_counts[token] += 1\n",
    "\n",
    "    \n",
    "    return word_counts\n",
    "\n",
    "def get_nplus_words(tokenized_sentences, n):\n",
    "    \"\"\"\n",
    "    Find words in corpus with nplus frequency\n",
    "    \"\"\"\n",
    "    nplus_vocab = []\n",
    "    \n",
    "    word_counts = count_words(tokenized_sentences)\n",
    "\n",
    "    for word, cnt in word_counts.items():\n",
    "        if cnt >= n:\n",
    "            nplus_vocab.append(word)\n",
    "    \n",
    "    return  nplus_vocab\n",
    "\n",
    "def replace_words_below_n_by_unk(tokenized_sentences, n=2):\n",
    "    \"\"\"\n",
    "    Process training data to replace words with frequency less than n by UNK\n",
    "    \"\"\"\n",
    "    vocabulary = set(get_nplus_words(tokenized_sentences, n))\n",
    "    \n",
    "    processed_tokenized_sentences = []\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        temp_sentence = []\n",
    "        \n",
    "        for token in sentence: \n",
    "            if token in vocabulary: # complete this line\n",
    "                temp_sentence.append(token)\n",
    "            else:\n",
    "                temp_sentence.append(\"<unk>\")\n",
    "        \n",
    "        processed_tokenized_sentences.append(temp_sentence)\n",
    "        \n",
    "    return processed_tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "23cae9da-ad74-4c23-9076-7f9bfc622500",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\User\\\\Downloads\\\\NLP ASS 2\\\\NLP_Spellchecker_App-master (1)\"\n",
    "dirs = os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7f2328f8-71eb-4d1a-a37e-9c5548573060",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"medical_vocab.txt\", \"r\", encoding = 'ISO-8859-1') as f: \n",
    "    medical_vocab = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c835ca4c-2cda-4a08-9799-5b65156ce7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_vocab = re.sub(r'[^A-Za-z\\.\\?!\\']+', ' ', medical_vocab) #remove special character\n",
    "medical_vocab = re.sub(r'\\b(?:[A-Z]{3,}\\b|(?:[A-Za-z]\\.){2,})\\s*', ' ',medical_vocab)# removes 3 or more capital letters e.g. XYZ or X.Y.Z. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1f4c10b6-dd81-44e0-aec4-c7f9e5428992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "medical_vocab = word_tokenize(medical_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9250f171-8543-4278-adb0-1ef0c3c74039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Vocabulary\n",
    "vocabulary = list(set(get_nplus_words(tokenized_sentences, 2))) #words with more than 2 frequency\n",
    "vocabulary = vocabulary + medical_vocab #append medical vocab to gutenberg vocab\n",
    "vocabulary = vocabulary+['<s>']+['<e>']\n",
    "# Replace less frequent word by <UNK>\n",
    "processed_sentences =replace_words_below_n_by_unk(tokenized_sentences, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c7741d9a-629b-42aa-8349-be3cdbd7aa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams_dict(tokenized_sentences, n, start_token='<s>', end_token = '<e>'):\n",
    "    \"\"\"\n",
    "    Return all n_grams with count\n",
    "    \"\"\"  \n",
    "    n_grams = {}\n",
    "\n",
    "    for sentence in tokenized_sentences: # complete this line\n",
    "        \n",
    "        # add start and end token\n",
    "        sentence = n * [start_token] + sentence + [end_token]\n",
    "        # convert to tuple\n",
    "        sentence = tuple(sentence)\n",
    "\n",
    "        for i in range(len(sentence)-n+1): \n",
    "\n",
    "            n_gram = sentence[i:i+n]\n",
    "            \n",
    "            if n_gram in n_grams:\n",
    "                n_grams[n_gram] += 1\n",
    "            else:\n",
    "                n_grams[n_gram] = 1\n",
    "    \n",
    "            ### END CODE HERE ###\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "79ff11fe-d2a8-4d25-a1ab-538e9df7d7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unigram and bigram\n",
    "unigram_counts = n_grams_dict(processed_sentences, 1)\n",
    "bigram_counts = n_grams_dict(processed_sentences, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "96bb757b-57e6-4b13-9cf7-2125f66aa5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def delete_letter(word, verbose=True):\n",
    "    '''return list of word with deleted letter\n",
    "    '''\n",
    "    \n",
    "    delete_l = []\n",
    "    split_l = []\n",
    "    \n",
    "    split_l = [(word[:i], word[i:]) for i in range(len(word)+1)]\n",
    "    delete_l = [L+R[1:] for L,R in split_l if R]\n",
    "\n",
    "    if verbose: print(f\"input word {word}, \\ndelete_l = {delete_l}\")\n",
    "\n",
    "    return delete_l\n",
    "\n",
    "def transpose_letter(word, verbose=False):\n",
    "    '''\n",
    "    return list of words with transposed letter\n",
    "    ''' \n",
    "    \n",
    "    transpose_l = []\n",
    "    split_l = []\n",
    "\n",
    "    split_l = [(word[:i], word[i:]) for i in range(len(word)+1)]\n",
    "    transpose_l = [L + R[1] + R[0] + R[2:] for L,R in split_l if len(R)>1]\n",
    "    \n",
    "    if verbose: print(f\"Input word = {word} \\ntranspose_l = {transpose_l}\") \n",
    "\n",
    "    return transpose_l\n",
    "\n",
    "def subst_letter(word, verbose=False):\n",
    "    '''\n",
    "    return list of substitute letter\n",
    "    ''' \n",
    "    \n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    subst_l = []\n",
    "    split_l = []\n",
    "\n",
    "    split_l = [(word[:i], word[i:]) for i in range(len(word)+1)]\n",
    "    subst_l = [L + l + R[1:] for L,R in split_l if R for l in letters]\n",
    "    subst_set = set(subst_l)    \n",
    "    subst_set.discard(word)\n",
    "\n",
    "    # turn the set back into a list and sort it, for easier viewing\n",
    "    subst_l = sorted(list(subst_set))\n",
    "    \n",
    "    if verbose: print(f\"Input word = {word} \\nsubst_l {subst_l}\")   \n",
    "    \n",
    "    return subst_l\n",
    "\n",
    "def ins_letter(word, verbose=False):\n",
    "    '''\n",
    "     return list of words with inserted letter\n",
    "    ''' \n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    ins_l = []\n",
    "    split_l = []\n",
    "    \n",
    "\n",
    "    split_l = [(word[:i], word[i:]) for i in range(len(word)+1)]\n",
    "    ins_l = [L + l + R for L,R in split_l for l in letters]\n",
    "    \n",
    "    if verbose: print(f\"Input word {word} \\nins_l = {ins_l}\")\n",
    "    \n",
    "    return ins_l\n",
    "\n",
    "def edit_one_letter(word):\n",
    "    \"\"\"\n",
    "    return a set of real/non word which is within edit distance of one \n",
    "    \"\"\"\n",
    "    edit_one_set = set(delete_letter(word) + transpose_letter(word) + subst_letter(word) + ins_letter(word))\n",
    "    \n",
    "    return edit_one_set\n",
    "\n",
    "def edit_two_letters(word):\n",
    "    '''\n",
    "    return a set of real/non word which is within edit distance of two\n",
    "    '''\n",
    "    \n",
    "    edit_one = edit_one_letter(word)\n",
    "    edit_two_set = set()\n",
    "    for w in edit_one:\n",
    "        edit_two = edit_one_letter(w)\n",
    "        edit_two_set = edit_two_set.union(edit_two_set, edit_two)\n",
    "   \n",
    "    return edit_two_set\n",
    "\n",
    "def min_edit_distance(source, target, ins_cost = 1, del_cost = 1, rep_cost = 1, trp_cost= 1):\n",
    "\n",
    "    # use deletion and insert cost as  1\n",
    "    m = len(source) \n",
    "    n = len(target) \n",
    "    #initialize cost matrix with zeros and dimensions (m+1,n+1) \n",
    "    D = np.zeros((m+1, n+1), dtype=int) \n",
    "    \n",
    "\n",
    "   # Fill in column 0, from row 1 to row m, both inclusive\n",
    "    for row in range(1,m+1): \n",
    "        D[row,0] = D[row-1, 0] + del_cost\n",
    "        \n",
    "    # Fill in row 0, for all columns from 1 to n, both inclusive\n",
    "    for col in range(1,n+1): \n",
    "        D[0,col] = D[0, col-1] + ins_cost\n",
    "                \n",
    "    # Loop through row 1 to row m, both inclusive\n",
    "    for row in range(1,m+1): \n",
    "        \n",
    "        # Loop through column 1 to column n, both inclusive\n",
    "        for col in range(1,n+1):\n",
    "            \n",
    "            # Intialize r_cost to the 'replace' cost that is passed into this function\n",
    "            r_cost = rep_cost\n",
    "            \n",
    "            # Check to see if source character at the previous row\n",
    "            # matches the target character at the previous column, \n",
    "            if source[row-1] == target[col-1]:\n",
    "                # Update the replacement cost to 0 if source and target are the same\n",
    "                r_cost = 0\n",
    "              \n",
    "            # Update the cost at row, col based on previous entries in the cost matrix\n",
    "            D[row,col] = min (D[row, col-1] + ins_cost, D[row-1, col] + del_cost, D[row-1, col-1] + r_cost)\n",
    "            # transposition  \n",
    "            if row>1 and col>1 and source[row-1]==target[col-2] and source[row-2] == target[col-1]:\n",
    "                D[row,col] = min (D[row,col], D[row-2,col-2] + trp_cost) \n",
    "    # Set the minimum edit distance with the cost found at row m, column n\n",
    "    min_distance = D[m,n]\n",
    "    \n",
    "    \n",
    "    return D, min_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ae322efe-915d-4739-b618-7705cc60ae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "f=open('addconfusion.data', 'r')\n",
    "data=f.read()\n",
    "f.close\n",
    "addmatrix=ast.literal_eval(data)\n",
    "f=open('subconfusion.data', 'r')\n",
    "data=f.read()\n",
    "f.close\n",
    "submatrix=ast.literal_eval(data)\n",
    "f=open('revconfusion.data', 'r')\n",
    "data=f.read()\n",
    "f.close\n",
    "revmatrix=ast.literal_eval(data)\n",
    "f=open('delconfusion.data', 'r')\n",
    "data=f.read()\n",
    "f.close\n",
    "delmatrix=ast.literal_eval(data)\n",
    "\n",
    "def editType(candidate, word):\n",
    "        \"Method to calculate edit type for single edit errors.\"\n",
    "        edit=[False]*4\n",
    "        correct=\"\"\n",
    "        error=\"\"\n",
    "        x=''\n",
    "        w=''\n",
    "        for i in range(min([len(word),len(candidate)])-1):\n",
    "            if candidate[0:i+1] != word[0:i+1]:\n",
    "                if candidate[i:] == word[i-1:]:\n",
    "                    edit[1]=True\n",
    "                    correct = candidate[i-1]\n",
    "                    error = ''\n",
    "                    x = candidate[i-2]\n",
    "                    w = candidate[i-2]+candidate[i-1]\n",
    "                    break\n",
    "                elif candidate[i:] == word[i+1:]:\n",
    "                    \n",
    "                    correct = ''\n",
    "                    error = word[i]\n",
    "                    if i == 0:\n",
    "                        w = '#'\n",
    "                        x = '#'+error\n",
    "                    else:\n",
    "                        w=word[i-1]\n",
    "                        x=word[i-1]+error\n",
    "                    edit[0]=True\n",
    "                    break\n",
    "                if candidate[i+1:] == word[i+1:]:\n",
    "                    edit[2]=True\n",
    "                    correct = candidate[i]\n",
    "                    error = word[i]\n",
    "                    x = error\n",
    "                    w = correct\n",
    "                    break\n",
    "                if candidate[i] == word[i+1] and candidate[i+2:]==word[i+2:]:\n",
    "                    edit[3]=True\n",
    "                    correct = candidate[i]+candidate[i+1]\n",
    "                    error = word[i]+word[i+1]\n",
    "                    x=error\n",
    "                    w=correct\n",
    "                    break\n",
    "        candidate=candidate[::-1]\n",
    "        word=word[::-1]\n",
    "        for i in range(min([len(word),len(candidate)])-1):\n",
    "            if candidate[0:i+1] != word[0:i+1]:\n",
    "                if candidate[i:] == word[i-1:]:\n",
    "                    edit[1]=True\n",
    "                    correct = candidate[i-1]\n",
    "                    error = ''\n",
    "                    x = candidate[i-2]\n",
    "                    w = candidate[i-2]+candidate[i-1]\n",
    "                    break\n",
    "                elif candidate[i:] == word[i+1:]:\n",
    "                    \n",
    "                    correct = ''\n",
    "                    error = word[i]\n",
    "                    if i == 0:\n",
    "                        w = '#'\n",
    "                        x = '#'+error\n",
    "                    else:\n",
    "                        w=word[i-1]\n",
    "                        x=word[i-1]+error\n",
    "                    edit[0]=True\n",
    "                    break\n",
    "                if candidate[i+1:] == word[i+1:]:\n",
    "                    edit[2]=True\n",
    "                    correct = candidate[i]\n",
    "                    error = word[i]\n",
    "                    x = error\n",
    "                    w = correct\n",
    "                    break\n",
    "                if candidate[i] == word[i+1] and candidate[i+2:]==word[i+2:]:\n",
    "                    edit[3]=True\n",
    "                    correct = candidate[i]+candidate[i+1]\n",
    "                    error = word[i]+word[i+1]\n",
    "                    x=error\n",
    "                    w=correct\n",
    "                    break\n",
    "        if word == candidate:\n",
    "            return \"None\", '', '', '', ''\n",
    "        if edit[1]:\n",
    "            return \"Deletion\", correct, error, x, w\n",
    "        elif edit[0]:\n",
    "            return \"Insertion\", correct, error, x, w\n",
    "        elif edit[2]:\n",
    "            return \"Substitution\", correct, error, x, w\n",
    "        elif edit[3]:\n",
    "            return \"Reversal\", correct, error, x, w\n",
    "        \n",
    "def channelModel(x,y, edit,corpus):\n",
    "        \"\"\"Method to calculate channel model probability for errors.\"\"\"\n",
    "        \n",
    "        if edit == 'add':\n",
    "            if x == '#':\n",
    "                return addmatrix[x+y]/corpus.count(' '+y)\n",
    "            else:\n",
    "                return addmatrix[x+y]/corpus.count(x)\n",
    "        if edit == 'sub':\n",
    "            return submatrix[(x+y)[0:2]]/corpus.count(y)\n",
    "        if edit == 'rev':\n",
    "            return revmatrix[x+y]/corpus.count(x+y)\n",
    "        if edit == 'del':\n",
    "            return delmatrix[x+y]/corpus.count(x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a1a978a1-3b56-4e7a-9ba4-5126576f1773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability(previous_n_words, word, \n",
    "                         previous_n_gram_dict, n_gram_dict, vocabulary_size, k=1.0):\n",
    "    \"\"\"\n",
    "    Return N-gram probability given the pair of current word and previous_n_words\n",
    "    \"\"\"\n",
    "    assert type(previous_n_words) == list\n",
    "    # convert list to tuple to use it as a dictionary key\n",
    "    previous_n_words = tuple(previous_n_words,)\n",
    "    \n",
    "    previous_n_words_count = previous_n_gram_dict[previous_n_words] if previous_n_words in previous_n_gram_dict else 0\n",
    "\n",
    "    # k-smoothing\n",
    "    denominator = previous_n_words_count + k*vocabulary_size\n",
    "\n",
    "    # Define n plus 1 gram as the previous n-gram plus the current word as a tuple\n",
    "    n_gram = previous_n_words + (word,)\n",
    "\n",
    "    n_gram_count = n_gram_dict[n_gram] if n_gram in n_gram_dict else 0\n",
    "   \n",
    "    # smoothing\n",
    "    numerator = n_gram_count + 1\n",
    "\n",
    "    probability = numerator/denominator\n",
    "    \n",
    "    \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "be9baaab-0740-4489-b689-3b68389a8e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corrections(previous_n_words_i, word, vocab, n=2, verbose = False):\n",
    "    '''\n",
    "    Get n candidates with individual probability\n",
    "    '''\n",
    "    assert type(previous_n_words_i) == list\n",
    "    corpus = ' '.join(vocabulary)\n",
    "    suggestions = []\n",
    "    n_best = []\n",
    "    ### Convert to UNK if word not in vocab\n",
    "    previous_n_words = []\n",
    "    for w in previous_n_words_i:\n",
    "        if w not in vocabulary:\n",
    "            previous_n_words.append('<unk>')\n",
    "        else:\n",
    "            previous_n_words.append(w)\n",
    "            \n",
    "    ##Suggestions include input word only if the input word in vocab\n",
    "    if word in vocab:    \n",
    "        suggestions = [word] + list(edit_one_letter(word).intersection(vocabulary)) or list(edit_two_letters(word).intersection(vocabulary)) \n",
    "    else:\n",
    "        suggestions = list(edit_one_letter(word).intersection(vocabulary)) or list(edit_two_letters(word).intersection(vocabulary)) \n",
    "        \n",
    "    words_prob = {}\n",
    "    for w in suggestions: \n",
    "        # To make sure all suggestions is within edit distance of 2\n",
    "        _, min_edits = min_edit_distance(' '.join(word),w)\n",
    "        if not word in vocab: ##use error model only when it is non word error\n",
    "            if min_edits <= 2:\n",
    "                edit = editType(w,' '.join(word))\n",
    "                if edit:##Some word cannot find edit\n",
    "                    if edit[0] == \"Insertion\":\n",
    "                        error_prob = channelModel(edit[3][0],edit[3][1], 'add',corpus)\n",
    "                    if edit[0] == 'Deletion':\n",
    "                        error_prob = channelModel(edit[4][0], edit[4][1], 'del',corpus)\n",
    "                    if edit[0] == 'Reversal':\n",
    "                        error_prob = channelModel(edit[4][0], edit[4][1], 'rev',corpus)\n",
    "                    if edit[0] == 'Substitution':\n",
    "                        error_prob = channelModel(edit[3], edit[4], 'sub',corpus)\n",
    "                else:\n",
    "                    error_prob = 1\n",
    "            else:\n",
    "                error_prob = 1\n",
    "        else:\n",
    "            error_prob = 1\n",
    "        language_prob = get_probability(previous_n_words, w, \n",
    "                        unigram_counts, bigram_counts, len(vocabulary), k=1.0)\n",
    "            \n",
    "        words_prob[w] = language_prob * error_prob\n",
    "        \n",
    "    n_best = Counter(words_prob).most_common(n)\n",
    "    \n",
    "    if verbose: print(\"entered word = \", word, \"\\nsuggestions = \", suggestions)\n",
    "\n",
    "    return n_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4078b576-1b5f-45f2-b1d6-483083c458f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GUI Creations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6679b759-7443-48b9-8f83-aebe7b610e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUI CREATION THROUGH PYTHON'S TKINTER LIBRARY\n",
    "from tkinter import *\n",
    "\n",
    "# creates a base GUI window\n",
    "root = Tk() \n",
    "\n",
    "# creating fixed geometry of the tkinter window with dimensions 700x900\n",
    "root.geometry(\"700x900\") \n",
    "root.configure(background = \"light blue\")\n",
    "\n",
    "root.title(\"NLP Spell Checker\") # Adding a title to the GUI window.\n",
    "Label(root, text = \"NLP Assignment 2\", fg = \"navy\", bg = \"gray\", font = \"Arial 11 bold italic\", height = 3, width = 200).pack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b34e316f-1861-41d6-b337-b843f827d39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve the sentence typed by a user & pass the input through get_corrections() to check spellings\n",
    "tokenized_sentence = []\n",
    "non_real_word = []\n",
    "\n",
    "clicked=StringVar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e5b24b24-0221-47cf-9463-f0589e9d2cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInput():\n",
    "    global tokenized_sentence\n",
    "    # Preprocess the original text input to get clean input\n",
    "    sentenceValues = enteredSentence.get('1.0', '50.0') #get input sentence\n",
    "    sentenceValues = sentenceValues.lower()\n",
    "    outputSentence.delete(0.0, 'end')\n",
    "    outputSentence.insert(END, sentenceValues)  \n",
    "    \n",
    "    # tokenize the sentence and save the values to tokenizedWords variable\n",
    "    tokenized_sentence = nltk.word_tokenize(sentenceValues)\n",
    "    tokenized_sentence = ['<s>']+ tokenized_sentence\n",
    "    \n",
    "    not_in_corpus=[]\n",
    "    real_word_error=[]\n",
    "    for word in tokenized_sentence:\n",
    "        if word not in vocabulary:\n",
    "            not_in_corpus.append(word)  # Saving non real word to not_in_corpus list.\n",
    "\n",
    "    for word in tokenized_sentence[1:]:\n",
    "        if word in vocabulary: \n",
    "            index=tokenized_sentence.index(word)\n",
    "            candidate_words = get_corrections([tokenized_sentence[index-1]], word, vocabulary, n=1, verbose=False)\n",
    "            if candidate_words[0][0] != word :\n",
    "                real_word_error.append(word) # saving a real & existing word to real_word_error\n",
    "    print(real_word_error)            \n",
    "    print(\"Suitable candidate words are:\")\n",
    "    \n",
    "    # Checking for non_word errors from the input sentence typed by a user\n",
    "    options=[]\n",
    "    for word in not_in_corpus:\n",
    "        \n",
    "        offset = '+%dc' % len(word) # +5c (5 chars)\n",
    "    \n",
    "        # search word from first char (1.0) to the end of text (END)\n",
    "        pos_start = enteredSentence.search(word, '1.0', END)\n",
    "        \n",
    "        # check if the word has been found\n",
    "        while pos_start:\n",
    "        \n",
    "            # create end position by adding (as string \"+5c\") number of chars in searched word \n",
    "            pos_end = pos_start + offset\n",
    "        \n",
    "            # add tag\n",
    "            enteredSentence.tag_add('red_tag', pos_start, pos_end)\n",
    "        \n",
    "            # search again from pos_end to the end of text (END)\n",
    "            pos_start = enteredSentence.search(word, pos_end, END)\n",
    "        \n",
    "        options.append(word)   \n",
    "    \n",
    "    # checking for real word error from the input sentence by a user\n",
    "    for word in real_word_error:\n",
    "        offset = '+%dc' % len(word) # +5c (5 chars)\n",
    "    \n",
    "        # search word from first char (1.0) to the end of text (END)\n",
    "        pos_start = enteredSentence.search(word, '1.0', END)\n",
    "        \n",
    "        # check if the word has been found\n",
    "        while pos_start:\n",
    "        \n",
    "            # create end position by adding (as string \"+5c\") number of chars in searched word \n",
    "            pos_end = pos_start + offset\n",
    "        \n",
    "            # add tag\n",
    "            enteredSentence.tag_add('blue_tag', pos_start, pos_end)\n",
    "        \n",
    "            # search again from pos_end to the end of text (END)\n",
    "            pos_start = enteredSentence.search(word, pos_end, END)\n",
    "        \n",
    "        options.append(word)   \n",
    "    \n",
    "    # Creating a drop down menu to display the misspelled words.\n",
    "    # From this drop down list, a user selects the misspelled word that they need suggestions for.\n",
    "    drop = OptionMenu(root,clicked,*options)\n",
    "    drop.configure(font=(\"Arial\", 10))\n",
    "    drop.pack()\n",
    "    drop.place(x=305, y = 350)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0e429dc8-c304-4633-b19f-4662ef06a241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display a list of the suggested words\n",
    "def showSuggestions():\n",
    "    suggestedWords.delete(0, END)\n",
    "    options=[]\n",
    "    word_to_replace = clicked.get()\n",
    "    index=tokenized_sentence.index(word_to_replace)\n",
    "    \n",
    "    candidate_words = get_corrections([tokenized_sentence[index-1]], word_to_replace, vocabulary, n=3, verbose=False)\n",
    "    print(candidate_words)\n",
    "    for i in range(len(candidate_words)):\n",
    "        suggestedWords.insert(END,candidate_words[i][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ff76f718-47a2-44cd-9ce6-a05966c09e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace a misspelled word with the correct word from a list of suggested words            \n",
    "def replace_word():\n",
    "    word_to_replace = clicked.get()\n",
    "    selected_word=suggestedWords.get(ANCHOR)\n",
    "    offset = '+%dc' % len(word_to_replace) # +5c (5 chars)\n",
    "    idx = '1.0'\n",
    "    #searches for desried string from index 1  \n",
    "    idx = outputSentence.search(word_to_replace, idx, nocase = 1,  \n",
    "                            stopindex = END) \n",
    "    # last index sum of current index and  \n",
    "    # length of text  \n",
    "    lastidx = '% s+% dc' % (idx, len(word_to_replace)) \n",
    "  \n",
    "    outputSentence.delete(idx, lastidx) \n",
    "    outputSentence.insert(idx, selected_word) \n",
    "  \n",
    "    lastidx = '% s+% dc' % (idx, len(selected_word))  \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "aba5f643-b96c-4454-9544-3c73c8739c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to highlight the listbox from keyword search \n",
    "def highlight():\n",
    "    searchx = SEARCH.get()\n",
    "    searchx = searchx.lower()\n",
    "    for i,item in enumerate (all_listbox_item):\n",
    "        if searchx in item:\n",
    "            listbox.selection_set(i) #highlights word\n",
    "            listbox.see(i) #scrolls down to word if not within the listbox\n",
    "        else:\n",
    "            listbox.selection_clear(i)\n",
    "        if searchx == '':\n",
    "            listbox.selection_clear(0,\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1ae97374-cb83-4761-ace0-223d7de8a886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clear the text in Searchbox    \n",
    "def Reset():\n",
    "    global SEARCH\n",
    "    clearinput = \"\"\n",
    "    SEARCH.set(clearinput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "25b9ec2e-0b00-49ab-9edb-25b64f1767e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to enable a pop up window on right click\n",
    "def do_popup(event): \n",
    "    try:\n",
    "        m.tk_popup(event.x_root, event.y_root)\n",
    "    finally:\n",
    "        m.grab_release()    \n",
    "\n",
    "# when the popup window opens on right click, it will have a word called suggest which will execute the showSuggestions function\n",
    "m = Menu(root, tearoff = 0) \n",
    "m.add_command(label =\"suggest\", command=showSuggestions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f3b22647-f923-491a-b1cb-86efff122bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH = StringVar()\n",
    "# Input widget for sentence to be entered by user\n",
    "Label(text=\"Enter sentence here (Max Words: 500)\", font=\"Arial 11 bold\").place(x=15, y=80)\n",
    "enteredSentence = Text(root, height = 10, width = 60)\n",
    "enteredSentence.configure(font=(\"Arial\", 11))\n",
    "enteredSentence.place(x=15, y=110)\n",
    "submit_btn = Button(root, height=1, width=10, text=\"Submit\", command=getInput).place(x=585, y=110)\n",
    "enteredSentence.tag_config(\"red_tag\", foreground=\"red\", underline=1)\n",
    "enteredSentence.tag_config(\"blue_tag\", foreground=\"blue\", underline=1)\n",
    "enteredSentence.tag_bind(\"red_tag\", '<Button-3>', do_popup) #calls the pop up funtion\n",
    "enteredSentence.tag_bind(\"blue_tag\", '<Button-3>', do_popup) #calls the pop up funtion\n",
    "\n",
    "# Creating a suggestions widget for the suggested words to correct the mispelled word\n",
    "Label(text=\"List of suggested words to replace misspelled word:\", font = \"Arial 11 bold\").place(x=15, y=320)\n",
    "suggestedWords = Listbox(root, height = 10, width = 30)\n",
    "suggestedWords.configure(font=(\"Arial\", 11))\n",
    "#suggestedWords.config(state = \"disabled\")\n",
    "suggestedWords.place(x=15, y = 350)\n",
    "sugg_btn = Button(root, text=\"Show suggestions\", command=showSuggestions).place(x=305, y=380)\n",
    "replace_btn = Button(root, text=\"Replace Word\", command=replace_word).place(x=305, y=410)\n",
    "\n",
    "# Output widget for the sentence entered and open for correcting mispelled words\n",
    "Label(text=\"Corrected Input Sentence by User:\", font = \"Arial 11 bold\").place(x=15, y=560)\n",
    "outputSentence = Text(root, height = 10, width = 60, wrap=WORD)\n",
    "outputSentence.configure(font=(\"Arial\", 11))\n",
    "#outputSentence.config(state = \"disabled\")\n",
    "outputSentence.place(x=15, y=590)\n",
    "\n",
    "#---------------------------------WIDGETS FOR DICTIONARY-----------------------------------\n",
    "\n",
    "#=====================================LABEL WIDGET=========================================\n",
    "lbl_title = Label(root, width=20, font=('arial', 8), text=\"Dictionary\").place(x=550, y=360) \n",
    "\n",
    "#=====================================ENTRY WIDGET=========================================\n",
    "search_entry = Entry(root, textvariable=SEARCH).place(x=550, y=530)\n",
    "\n",
    "#=====================================BUTTON WIDGET========================================\n",
    "btn_search = Button(root, text=\"Search\", bg=\"#006dcc\", command=highlight).place(x=550, y=555) \n",
    "btn_reset = Button(root=search_entry, text=\"Reset\", bg=\"#ff0000\",command=Reset).place(x=635, y=555) \n",
    "\n",
    "#=====================================Table WIDGET=========================================\n",
    "#highlight the selection in listbox\n",
    "listbox = Listbox(root, height=9)\n",
    "selection = listbox.curselection()\n",
    "listbox.place(x=550,y=380) \n",
    "\n",
    "#upload txt to listbox and populate it\n",
    "f = vocabulary\n",
    "\n",
    "for x in f:\n",
    "    listbox.insert(\"end\", x)\n",
    "\n",
    "all_listbox_item = listbox.get(0, \"end\")\n",
    "\n",
    "\n",
    "# Activating the GUI\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23496e04-1367-4854-8b78-70aceb006b04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
